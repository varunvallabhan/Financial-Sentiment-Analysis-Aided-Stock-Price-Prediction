{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"NLP.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"GRwv4Z7ciaXk","colab_type":"text"},"source":["This is the NLP segment and deals with only converting the news to sentiment values and saving them along the way. The Phase two of this the the stock prediction which is the second notebook in the folder which has the codde which merges the output of this with the stock data and trains the LSTM model."]},{"cell_type":"code","metadata":{"id":"GMqvqHvQKRIi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1593417664530,"user_tz":240,"elapsed":19920,"user":{"displayName":"varun vallabhan","photoUrl":"","userId":"05449009114552710154"}},"outputId":"0a879d86-8612-4f0d-9900-0a29ca643fe1"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ios09osGJobN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1593417671244,"user_tz":240,"elapsed":3400,"user":{"displayName":"varun vallabhan","photoUrl":"","userId":"05449009114552710154"}},"outputId":"726d63bc-198c-4bb4-c195-b9908665b111"},"source":["import os\n","import pandas as pd\n","import numpy as np\n","np.random.seed(7)\n","import nltk\n","nltk.download('vader_lexicon')\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n","from sklearn import svm\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Dense, Input\n","from keras.layers import Conv1D, GlobalMaxPooling1D,Embedding, Dropout,BatchNormalization\n","from xgboost import XGBClassifier\n","from keras.callbacks import ModelCheckpoint\n","from keras.optimizers import Adam"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n","  warnings.warn(\"The twython library has not been installed. \"\n","Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"CEHmJVHfJobb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1593417675927,"user_tz":240,"elapsed":3657,"user":{"displayName":"varun vallabhan","photoUrl":"","userId":"05449009114552710154"}},"outputId":"52d23d50-1fd8-4d3c-9a63-1228bff41955"},"source":["class data_manager:\n","  \"\"\"\n","  This segment here imports the data. \n","  It is designed to either import data using \n","  pandas or numpy depending on the typr of file.\n","\n","  \n","\n","\n","  \"\"\"\n","  \n","  \n","  def fetch_data(self,path_to_data='',numpy=False):\n","    \"\"\"\n","    This function fetches the data using pandas or numpy depending on the Value set\n","    and returns a data. It takes an additional string which is the location of the file to import the data.\n","    \n","    path_to_data : String \n","    the directory of the file that needs to be fetched.\n","\n","    numpy : Boolean\n","    When true it loads numpy data. Default is False to load the csv data using pandas.\n","    \"\"\"\n","    if numpy:\n","      return np.load(path_to_data)\n","    else:\n","      return pd.read_csv(path_to_data,low_memory=False,parse_dates=[0])\n","  def saver(self,file_to_save,path_to_save=''):\n","    np.save(path_to_save,file_to_save)\n","\n","\n","\n","importer=data_manager()\n","df=importer.fetch_data(\"drive/My Drive/Summer 2020/Finance/data/stocknews/Combined_News_DJIA.csv\")\n","print(df.head(5))\n","#df = pd.read_csv(\"drive/My Drive/Summer 2020/Finance/data/stocknews/Combined_News_DJIA.csv\",low_memory=False, parse_dates=[0])\n","\n","full_stock = importer.fetch_data(\"drive/My Drive/Summer 2020/Finance/data/stocknews/upload_DJIA_table.csv\")\n","\n","#add the closing stock value to the df - this will be the y variable\n","df[\"Close\"]=full_stock.Close\n","\n","#show how the dataset looks like\n","#df.head(5)\n","print(df.shape)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["        Date  ...                                              Top25\n","0 2008-08-08  ...           b\"No Help for Mexico's Kidnapping Surge\"\n","1 2008-08-11  ...  b\"So this is what it's come to: trading sex fo...\n","2 2008-08-12  ...  b\"BBC NEWS | Asia-Pacific | Extinction 'by man...\n","3 2008-08-13  ...  b'2006: Nobel laureate Aleksander Solzhenitsyn...\n","4 2008-08-14  ...  b'Philippines : Peace Advocate say Muslims nee...\n","\n","[5 rows x 27 columns]\n","(1989, 28)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"APWOqJVmJobn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":862},"executionInfo":{"status":"ok","timestamp":1593417680643,"user_tz":240,"elapsed":523,"user":{"displayName":"varun vallabhan","photoUrl":"","userId":"05449009114552710154"}},"outputId":"a2ad69db-ea90-48d4-8843-046680121245"},"source":["\"\"\"\n","This segment deals with cleaning the data. \n","The web scrapped data leads to unwanted characters which \n","causes unwanted characters which are cleaned in this segment.\n","We replace nan with empty string and we count the number of nans \n","in the data to do a sanity check to make sure all nans are removed.\n","\"\"\"\n","\n","nlp=df['Label']\n","nlp=nlp.to_numpy()  # we strip the labels and take it out if the equation before we clean the data.\n","df = df.drop([\"Label\"], axis=1)\n","print (df.isnull().sum())\n","df = df.replace(np.nan, ' ', regex=True)\n","scaler=MinMaxScaler(feature_range=(-1,1))\n","\n","#sanity check\n","print(df.isnull().sum().sum())\n","df = df.replace('b\\\"|b\\'|\\\\\\\\|\\\\\\\"', '', regex=True)\n","df.head(2)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Date     0\n","Top1     0\n","Top2     0\n","Top3     0\n","Top4     0\n","Top5     0\n","Top6     0\n","Top7     0\n","Top8     0\n","Top9     0\n","Top10    0\n","Top11    0\n","Top12    0\n","Top13    0\n","Top14    0\n","Top15    0\n","Top16    0\n","Top17    0\n","Top18    0\n","Top19    0\n","Top20    0\n","Top21    0\n","Top22    0\n","Top23    1\n","Top24    3\n","Top25    3\n","Close    0\n","dtype: int64\n","0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Top1</th>\n","      <th>Top2</th>\n","      <th>Top3</th>\n","      <th>Top4</th>\n","      <th>Top5</th>\n","      <th>Top6</th>\n","      <th>Top7</th>\n","      <th>Top8</th>\n","      <th>Top9</th>\n","      <th>Top10</th>\n","      <th>Top11</th>\n","      <th>Top12</th>\n","      <th>Top13</th>\n","      <th>Top14</th>\n","      <th>Top15</th>\n","      <th>Top16</th>\n","      <th>Top17</th>\n","      <th>Top18</th>\n","      <th>Top19</th>\n","      <th>Top20</th>\n","      <th>Top21</th>\n","      <th>Top22</th>\n","      <th>Top23</th>\n","      <th>Top24</th>\n","      <th>Top25</th>\n","      <th>Close</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2008-08-08</td>\n","      <td>Georgia 'downs two Russian warplanes' as count...</td>\n","      <td>BREAKING: Musharraf to be impeached.'</td>\n","      <td>Russia Today: Columns of troops roll into Sout...</td>\n","      <td>Russian tanks are moving towards the capital o...</td>\n","      <td>Afghan children raped with 'impunity,' U.N. of...</td>\n","      <td>150 Russian tanks have entered South Ossetia w...</td>\n","      <td>Breaking: Georgia invades South Ossetia, Russi...</td>\n","      <td>The 'enemy combatent' trials are nothing but a...</td>\n","      <td>Georgian troops retreat from S. Osettain capit...</td>\n","      <td>Did the U.S. Prep Georgia for War with Russia?'</td>\n","      <td>Rice Gives Green Light for Israel to Attack Ir...</td>\n","      <td>Announcing:Class Action Lawsuit on Behalf of A...</td>\n","      <td>So---Russia and Georgia are at war and the NYT...</td>\n","      <td>China tells Bush to stay out of other countrie...</td>\n","      <td>Did World War III start today?'</td>\n","      <td>Georgia Invades South Ossetia - if Russia gets...</td>\n","      <td>Al-Qaeda Faces Islamist Backlash'</td>\n","      <td>Condoleezza Rice: The US would not act to prev...</td>\n","      <td>This is a busy day:  The European Union has ap...</td>\n","      <td>Georgia will withdraw 1,000 soldiers from Iraq...</td>\n","      <td>Why the Pentagon Thinks Attacking Iran is a Ba...</td>\n","      <td>Caucasus in crisis: Georgia invades South Osse...</td>\n","      <td>Indian shoe manufactory  - And again in a seri...</td>\n","      <td>Visitors Suffering from Mental Illnesses Banne...</td>\n","      <td>No Help for Mexico's Kidnapping Surge</td>\n","      <td>11734.32031</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2008-08-11</td>\n","      <td>Why wont America and Nato help us? If they won...</td>\n","      <td>Bush puts foot down on Georgian conflict'</td>\n","      <td>Jewish Georgian minister: Thanks to Israeli tr...</td>\n","      <td>Georgian army flees in disarray as Russians ad...</td>\n","      <td>Olympic opening ceremony fireworks 'faked'</td>\n","      <td>What were the Mossad with fraudulent New Zeala...</td>\n","      <td>Russia angered by Israeli military sale to Geo...</td>\n","      <td>An American citizen living in S.Ossetia blames...</td>\n","      <td>Welcome To World War IV! Now In High Definition!'</td>\n","      <td>Georgia's move, a mistake of monumental propor...</td>\n","      <td>Russia presses deeper into Georgia; U.S. says ...</td>\n","      <td>Abhinav Bindra wins first ever Individual Olym...</td>\n","      <td>U.S. ship heads for Arctic to define territory'</td>\n","      <td>Drivers in a Jerusalem taxi station threaten t...</td>\n","      <td>The French Team is Stunned by Phelps and the 4...</td>\n","      <td>Israel and the US behind the Georgian aggressi...</td>\n","      <td>Do not believe TV, neither Russian nor Georgia...</td>\n","      <td>Riots are still going on in Montreal (Canada) ...</td>\n","      <td>China to overtake US as largest manufacturer'</td>\n","      <td>War in South Ossetia [PICS]'</td>\n","      <td>Israeli Physicians Group Condemns State Torture'</td>\n","      <td>Russia has just beaten the United States over...</td>\n","      <td>Perhaps *the* question about the Georgia - Rus...</td>\n","      <td>Russia is so much better at war'</td>\n","      <td>So this is what it's come to: trading sex for ...</td>\n","      <td>11782.34961</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Date  ...        Close\n","0 2008-08-08  ...  11734.32031\n","1 2008-08-11  ...  11782.34961\n","\n","[2 rows x 27 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"7xxyAoJKJoby","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":392},"executionInfo":{"status":"error","timestamp":1592811212596,"user_tz":240,"elapsed":8747,"user":{"displayName":"varun vallabhan","photoUrl":"","userId":"05449009114552710154"}},"outputId":"ffd53595-c6f1-4431-c4d4-2dc1c220d8ec"},"source":["\n","class sentiment:\n","  \"\"\"\n","  This segment deals with getting the NLP sentiment values for each headline. \n","  We traverse through earch day and each of the 25 headlines for the day and \n","  calculte the sentiment value. then we generate a tabular data with just \n","  the compund values for each news for each day. we then save the calulated \n","  values for being used for MLP prediction.\n","\n","  The function class provides us with a numpy data array with the compound values for each news chunk.\n","  \n","\n","  news_data : pandas dataframe\n","  \"\"\"\n","  def __init__(self,news_data):\n","    self.data=news_data\n","  def sentiment_analysis(self):\n","    results=[]\n","    for y in range(1,26):\n","        res=[]\n","        print(y)\n","        for x in self.data['Top'+str(y)]:\n","            \n","            pol_score = SIA().polarity_scores(x) # run analysis\n","            \n","            res.append(pol_score['compound'])\n","            \n","        results.append(res)\n","\n","    print(results)\n","    sentiment_values=np.array(results)  \n","    return sentiment_values\n","sentiment_analyzer=sentiment(df)\n","NLP_values=sentiment_analyzer.sentiment_analysis()\n","#importer.saver(\"C:/Users/byakuya/Downloads/stocknews/nlp_compound.npy\",NLP_inputs)\n","\n","\n","\n","'''results=[]\n","for y in range(1,26):\n","    res=[]\n","    print(y)\n","    for x in df['Top'+str(y)]:\n","        \n","        pol_score = SIA().polarity_scores(x) # run analysis\n","        \n","        res.append(pol_score['compound'])\n","        \n","    results.append(res)\n","\n","print(results)\n","NLP_inputs=np.array(results)  \n","np.save(\"C:/Users/byakuya/Downloads/stocknews/nlp_compound.npy\",NLP_inputs)'''\n","#np.save('C:/Users/byakuya/Downloads/stocknews/nlp_values.npy',nlp) # this was to check the output of the actual labels."],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-e18678488174>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentiment_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0msentiment_analyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mNLP_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentiment_analyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;31m#importer.saver(\"C:/Users/byakuya/Downloads/stocknews/nlp_compound.npy\",NLP_inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-34-e18678488174>\u001b[0m in \u001b[0;36msentiment_analysis\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Top'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mpol_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSIA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpol_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compound'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lexicon_file)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexicon_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_lex_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_lex_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36mmake_lex_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \"\"\"\n\u001b[1;32m    207\u001b[0m         \u001b[0mlex_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mlex_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"WqVOstujJob4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593417688751,"user_tz":240,"elapsed":1687,"user":{"displayName":"varun vallabhan","photoUrl":"","userId":"05449009114552710154"}},"outputId":"2e297f0e-5897-4c4f-a5d1-8e6fd0e9f151"},"source":["\"\"\"\n","First we import the compound values. \n","The path is the current path for the NLP\n","sentiment values for each day stored in the drive.\n","We then scale and split the data to test and train. the current split is 60% train and 40% Test.\n","\"\"\"\n","\n","dat=importer.fetch_data(\"drive/My Drive/Summer 2020/Finance/data/stocknews/nlp_compound.npy\",numpy=True).T\n","scaler.fit(dat)\n","X_train, X_test, y_train, y_test = train_test_split( dat, nlp, test_size=0.4)\n","X_train=scaler.transform(X_train)\n","X_test=scaler.transform(X_test)\n","\n","\n","\"\"\"\n","Just for comparison I also used random forests \n","to predict the values and print its final score.\n","\"\"\"\n","clf = RandomForestClassifier(max_depth=12 )\n","clf.fit(X_train,y_train)\n","print (clf.score(X_test,y_test))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["0.5251256281407035\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OHOYYitFTIIh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1592811239453,"user_tz":240,"elapsed":17205,"user":{"displayName":"varun vallabhan","photoUrl":"","userId":"05449009114552710154"}},"outputId":"4219b630-848a-41b6-8ec5-8ba89120a012"},"source":["\"\"\"\n","Similarly, I also used Extreme Gradient boosting (XGBoost) for comapring with my model.\n","\"\"\"\n","param_dist = {'objective':'binary:logistic', 'n_estimators':3000,'max_depth':20,'learning_rate':0.01}\n","\n","clf = XGBClassifier(**param_dist)\n","\n","clf.fit(X_train, y_train,\n","        \n","        verbose=True)\n","\n","#evals_result = clf.evals_result()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","              colsample_bynode=1, colsample_bytree=1, gamma=0,\n","              learning_rate=0.01, max_delta_step=0, max_depth=20,\n","              min_child_weight=1, missing=None, n_estimators=3000, n_jobs=1,\n","              nthread=None, objective='binary:logistic', random_state=0,\n","              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","              silent=None, subsample=1, verbosity=1)"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"Ep9B2cDIO2o1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"status":"ok","timestamp":1592811248013,"user_tz":240,"elapsed":1466,"user":{"displayName":"varun vallabhan","photoUrl":"","userId":"05449009114552710154"}},"outputId":"a7e16a7d-5790-4c7d-96d1-378621b3b6eb"},"source":["\"\"\"\n","A part of XGBoost.\n","\"\"\"\n","X_test.shape\n","print(np.sum(y_test),len(y_test))\n","out=clf.predict(X_test)\n","out1=clf.predict(X_train)\n","print(clf.score(X_test,y_test))\n","#np.save('drive/My Drive/downloads/stocknews/out_test.npy',out)\n","#np.save('drive/My Drive/downloads/stocknews/out_train.npy',out1)\n","print(out )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["424 796\n","0.4949748743718593\n","[1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n"," 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1\n"," 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n"," 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0\n"," 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1\n"," 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1\n"," 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n"," 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0\n"," 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0\n"," 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1\n"," 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1\n"," 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1\n"," 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0\n"," 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1\n"," 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1 1\n"," 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0\n"," 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1\n"," 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1\n"," 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 1 1 0\n"," 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1\n"," 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1\n"," 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NTT_Nn5qO2eq","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"cRnSALZC-5Du","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593418607556,"user_tz":240,"elapsed":7060,"user":{"displayName":"varun vallabhan","photoUrl":"","userId":"05449009114552710154"}},"outputId":"a0319448-8475-4cd1-da89-3d44bafbb699"},"source":["from keras.callbacks import ModelCheckpoint\n","from tensorflow.keras import regularizers\n","import tensorflow as tf\n","from keras.layers.advanced_activations import LeakyReLU\n","from tensorflow import keras\n","loss_fn = keras.losses.BinaryCrossentropy(label_smoothing=0.01)\n","model = Sequential()\n","tf.random.set_seed(0)\n","#initializer = tf.keras.initializers.GlorotUniform()\n","#model.add(Dropout(0.2))\n","model.add(Dense(16, input_dim=25))\n","model.add(LeakyReLU(0.1))\n","model.add(Dropout(0.2))\n","#model.add(BatchNormalization())\n","model.add(Dense(32))\n","model.add(LeakyReLU(0.1))\n","model.add(Dropout(0.2))\n","model.add(BatchNormalization())\n","model.add(Dense(64))\n","model.add(LeakyReLU(0.1))\n","model.add(Dropout(.2))\n","model.add(BatchNormalization())\n","model.add(Dense(32))\n","model.add(LeakyReLU(0.1))\n","model.add(Dropout(0.2))\n","model.add(BatchNormalization())\n","model.add(Dense(16))\n","model.add(LeakyReLU(0.1))\n","model.add(Dropout(0.2))\n","model.add(BatchNormalization())\n","model.add(Dense(8))\n","model.add(LeakyReLU(0.1))\n","model.add(Dropout(0.2))\n","model.add(BatchNormalization())\n","model.add(Dense(5))\n","model.add(LeakyReLU(0.1))\n","model.add(Dense(1,activation='sigmoid'))\n","\n","model.compile(loss=loss_fn, optimizer=Adam(learning_rate=0.0005),metrics=[\"accuracy\"])\n","filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n","callbacks_list = [checkpoint]\n","print(model.summary())\n","model.fit(X_train,y_train,epochs=100,validation_split=0.2,batch_size=128,callbacks=callbacks_list,shuffle=False,)\n","print(model.evaluate(X_test,y_test,batch_size=128))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Model: \"sequential_7\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_49 (Dense)             (None, 16)                416       \n","_________________________________________________________________\n","leaky_re_lu_43 (LeakyReLU)   (None, 16)                0         \n","_________________________________________________________________\n","dropout_37 (Dropout)         (None, 16)                0         \n","_________________________________________________________________\n","dense_50 (Dense)             (None, 32)                544       \n","_________________________________________________________________\n","leaky_re_lu_44 (LeakyReLU)   (None, 32)                0         \n","_________________________________________________________________\n","dropout_38 (Dropout)         (None, 32)                0         \n","_________________________________________________________________\n","batch_normalization_31 (Batc (None, 32)                128       \n","_________________________________________________________________\n","dense_51 (Dense)             (None, 64)                2112      \n","_________________________________________________________________\n","leaky_re_lu_45 (LeakyReLU)   (None, 64)                0         \n","_________________________________________________________________\n","dropout_39 (Dropout)         (None, 64)                0         \n","_________________________________________________________________\n","batch_normalization_32 (Batc (None, 64)                256       \n","_________________________________________________________________\n","dense_52 (Dense)             (None, 32)                2080      \n","_________________________________________________________________\n","leaky_re_lu_46 (LeakyReLU)   (None, 32)                0         \n","_________________________________________________________________\n","dropout_40 (Dropout)         (None, 32)                0         \n","_________________________________________________________________\n","batch_normalization_33 (Batc (None, 32)                128       \n","_________________________________________________________________\n","dense_53 (Dense)             (None, 16)                528       \n","_________________________________________________________________\n","leaky_re_lu_47 (LeakyReLU)   (None, 16)                0         \n","_________________________________________________________________\n","dropout_41 (Dropout)         (None, 16)                0         \n","_________________________________________________________________\n","batch_normalization_34 (Batc (None, 16)                64        \n","_________________________________________________________________\n","dense_54 (Dense)             (None, 8)                 136       \n","_________________________________________________________________\n","leaky_re_lu_48 (LeakyReLU)   (None, 8)                 0         \n","_________________________________________________________________\n","dropout_42 (Dropout)         (None, 8)                 0         \n","_________________________________________________________________\n","batch_normalization_35 (Batc (None, 8)                 32        \n","_________________________________________________________________\n","dense_55 (Dense)             (None, 5)                 45        \n","_________________________________________________________________\n","leaky_re_lu_49 (LeakyReLU)   (None, 5)                 0         \n","_________________________________________________________________\n","dense_56 (Dense)             (None, 1)                 6         \n","=================================================================\n","Total params: 6,475\n","Trainable params: 6,171\n","Non-trainable params: 304\n","_________________________________________________________________\n","None\n","Train on 954 samples, validate on 239 samples\n","Epoch 1/100\n","954/954 [==============================] - 1s 833us/step - loss: 0.7554 - accuracy: 0.4832 - val_loss: 0.6921 - val_accuracy: 0.5272\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.52720, saving model to weights-improvement-01-0.53.hdf5\n","Epoch 2/100\n","954/954 [==============================] - 0s 32us/step - loss: 0.7603 - accuracy: 0.4990 - val_loss: 0.6910 - val_accuracy: 0.5481\n","\n","Epoch 00002: val_accuracy improved from 0.52720 to 0.54812, saving model to weights-improvement-02-0.55.hdf5\n","Epoch 3/100\n","954/954 [==============================] - 0s 33us/step - loss: 0.7702 - accuracy: 0.4927 - val_loss: 0.6906 - val_accuracy: 0.5690\n","\n","Epoch 00003: val_accuracy improved from 0.54812 to 0.56904, saving model to weights-improvement-03-0.57.hdf5\n","Epoch 4/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7435 - accuracy: 0.4874 - val_loss: 0.6900 - val_accuracy: 0.5690\n","\n","Epoch 00004: val_accuracy did not improve from 0.56904\n","Epoch 5/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7575 - accuracy: 0.4759 - val_loss: 0.6896 - val_accuracy: 0.5732\n","\n","Epoch 00005: val_accuracy improved from 0.56904 to 0.57322, saving model to weights-improvement-05-0.57.hdf5\n","Epoch 6/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.7555 - accuracy: 0.4864 - val_loss: 0.6894 - val_accuracy: 0.5816\n","\n","Epoch 00006: val_accuracy improved from 0.57322 to 0.58159, saving model to weights-improvement-06-0.58.hdf5\n","Epoch 7/100\n","954/954 [==============================] - 0s 34us/step - loss: 0.7462 - accuracy: 0.5000 - val_loss: 0.6893 - val_accuracy: 0.5690\n","\n","Epoch 00007: val_accuracy did not improve from 0.58159\n","Epoch 8/100\n","954/954 [==============================] - 0s 38us/step - loss: 0.7401 - accuracy: 0.5021 - val_loss: 0.6891 - val_accuracy: 0.5732\n","\n","Epoch 00008: val_accuracy did not improve from 0.58159\n","Epoch 9/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7202 - accuracy: 0.5021 - val_loss: 0.6891 - val_accuracy: 0.5690\n","\n","Epoch 00009: val_accuracy did not improve from 0.58159\n","Epoch 10/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7272 - accuracy: 0.5010 - val_loss: 0.6891 - val_accuracy: 0.5732\n","\n","Epoch 00010: val_accuracy did not improve from 0.58159\n","Epoch 11/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7360 - accuracy: 0.5126 - val_loss: 0.6890 - val_accuracy: 0.5565\n","\n","Epoch 00011: val_accuracy did not improve from 0.58159\n","Epoch 12/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7352 - accuracy: 0.4874 - val_loss: 0.6890 - val_accuracy: 0.5732\n","\n","Epoch 00012: val_accuracy did not improve from 0.58159\n","Epoch 13/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7187 - accuracy: 0.5199 - val_loss: 0.6891 - val_accuracy: 0.5607\n","\n","Epoch 00013: val_accuracy did not improve from 0.58159\n","Epoch 14/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7249 - accuracy: 0.4864 - val_loss: 0.6889 - val_accuracy: 0.5649\n","\n","Epoch 00014: val_accuracy did not improve from 0.58159\n","Epoch 15/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7265 - accuracy: 0.5168 - val_loss: 0.6885 - val_accuracy: 0.5690\n","\n","Epoch 00015: val_accuracy did not improve from 0.58159\n","Epoch 16/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7158 - accuracy: 0.5241 - val_loss: 0.6882 - val_accuracy: 0.5690\n","\n","Epoch 00016: val_accuracy did not improve from 0.58159\n","Epoch 17/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7162 - accuracy: 0.5136 - val_loss: 0.6880 - val_accuracy: 0.5649\n","\n","Epoch 00017: val_accuracy did not improve from 0.58159\n","Epoch 18/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7115 - accuracy: 0.5189 - val_loss: 0.6879 - val_accuracy: 0.5607\n","\n","Epoch 00018: val_accuracy did not improve from 0.58159\n","Epoch 19/100\n","954/954 [==============================] - 0s 31us/step - loss: 0.7277 - accuracy: 0.5073 - val_loss: 0.6878 - val_accuracy: 0.5607\n","\n","Epoch 00019: val_accuracy did not improve from 0.58159\n","Epoch 20/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7134 - accuracy: 0.5157 - val_loss: 0.6876 - val_accuracy: 0.5607\n","\n","Epoch 00020: val_accuracy did not improve from 0.58159\n","Epoch 21/100\n","954/954 [==============================] - 0s 32us/step - loss: 0.7103 - accuracy: 0.5094 - val_loss: 0.6874 - val_accuracy: 0.5565\n","\n","Epoch 00021: val_accuracy did not improve from 0.58159\n","Epoch 22/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7121 - accuracy: 0.5084 - val_loss: 0.6872 - val_accuracy: 0.5439\n","\n","Epoch 00022: val_accuracy did not improve from 0.58159\n","Epoch 23/100\n","954/954 [==============================] - 0s 33us/step - loss: 0.7014 - accuracy: 0.5231 - val_loss: 0.6871 - val_accuracy: 0.5439\n","\n","Epoch 00023: val_accuracy did not improve from 0.58159\n","Epoch 24/100\n","954/954 [==============================] - 0s 31us/step - loss: 0.7079 - accuracy: 0.5367 - val_loss: 0.6869 - val_accuracy: 0.5356\n","\n","Epoch 00024: val_accuracy did not improve from 0.58159\n","Epoch 25/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7068 - accuracy: 0.5168 - val_loss: 0.6867 - val_accuracy: 0.5314\n","\n","Epoch 00025: val_accuracy did not improve from 0.58159\n","Epoch 26/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7193 - accuracy: 0.4853 - val_loss: 0.6866 - val_accuracy: 0.5356\n","\n","Epoch 00026: val_accuracy did not improve from 0.58159\n","Epoch 27/100\n","954/954 [==============================] - 0s 31us/step - loss: 0.7044 - accuracy: 0.5440 - val_loss: 0.6865 - val_accuracy: 0.5314\n","\n","Epoch 00027: val_accuracy did not improve from 0.58159\n","Epoch 28/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7062 - accuracy: 0.4969 - val_loss: 0.6866 - val_accuracy: 0.5397\n","\n","Epoch 00028: val_accuracy did not improve from 0.58159\n","Epoch 29/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7056 - accuracy: 0.5377 - val_loss: 0.6869 - val_accuracy: 0.5397\n","\n","Epoch 00029: val_accuracy did not improve from 0.58159\n","Epoch 30/100\n","954/954 [==============================] - 0s 32us/step - loss: 0.6993 - accuracy: 0.5472 - val_loss: 0.6867 - val_accuracy: 0.5397\n","\n","Epoch 00030: val_accuracy did not improve from 0.58159\n","Epoch 31/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6951 - accuracy: 0.5304 - val_loss: 0.6866 - val_accuracy: 0.5439\n","\n","Epoch 00031: val_accuracy did not improve from 0.58159\n","Epoch 32/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.6973 - accuracy: 0.5231 - val_loss: 0.6867 - val_accuracy: 0.5439\n","\n","Epoch 00032: val_accuracy did not improve from 0.58159\n","Epoch 33/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7028 - accuracy: 0.5283 - val_loss: 0.6871 - val_accuracy: 0.5439\n","\n","Epoch 00033: val_accuracy did not improve from 0.58159\n","Epoch 34/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7015 - accuracy: 0.5210 - val_loss: 0.6873 - val_accuracy: 0.5439\n","\n","Epoch 00034: val_accuracy did not improve from 0.58159\n","Epoch 35/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6930 - accuracy: 0.5535 - val_loss: 0.6871 - val_accuracy: 0.5439\n","\n","Epoch 00035: val_accuracy did not improve from 0.58159\n","Epoch 36/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.6961 - accuracy: 0.5325 - val_loss: 0.6871 - val_accuracy: 0.5481\n","\n","Epoch 00036: val_accuracy did not improve from 0.58159\n","Epoch 37/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.7135 - accuracy: 0.5052 - val_loss: 0.6875 - val_accuracy: 0.5523\n","\n","Epoch 00037: val_accuracy did not improve from 0.58159\n","Epoch 38/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6939 - accuracy: 0.5472 - val_loss: 0.6878 - val_accuracy: 0.5523\n","\n","Epoch 00038: val_accuracy did not improve from 0.58159\n","Epoch 39/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6954 - accuracy: 0.5398 - val_loss: 0.6881 - val_accuracy: 0.5523\n","\n","Epoch 00039: val_accuracy did not improve from 0.58159\n","Epoch 40/100\n","954/954 [==============================] - 0s 33us/step - loss: 0.7071 - accuracy: 0.5063 - val_loss: 0.6881 - val_accuracy: 0.5649\n","\n","Epoch 00040: val_accuracy did not improve from 0.58159\n","Epoch 41/100\n","954/954 [==============================] - 0s 33us/step - loss: 0.6898 - accuracy: 0.5545 - val_loss: 0.6880 - val_accuracy: 0.5607\n","\n","Epoch 00041: val_accuracy did not improve from 0.58159\n","Epoch 42/100\n","954/954 [==============================] - 0s 32us/step - loss: 0.7077 - accuracy: 0.4979 - val_loss: 0.6881 - val_accuracy: 0.5607\n","\n","Epoch 00042: val_accuracy did not improve from 0.58159\n","Epoch 43/100\n","954/954 [==============================] - 0s 33us/step - loss: 0.7011 - accuracy: 0.5241 - val_loss: 0.6881 - val_accuracy: 0.5565\n","\n","Epoch 00043: val_accuracy did not improve from 0.58159\n","Epoch 44/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.7056 - accuracy: 0.5220 - val_loss: 0.6884 - val_accuracy: 0.5523\n","\n","Epoch 00044: val_accuracy did not improve from 0.58159\n","Epoch 45/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.7052 - accuracy: 0.5084 - val_loss: 0.6885 - val_accuracy: 0.5523\n","\n","Epoch 00045: val_accuracy did not improve from 0.58159\n","Epoch 46/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.6925 - accuracy: 0.5241 - val_loss: 0.6886 - val_accuracy: 0.5523\n","\n","Epoch 00046: val_accuracy did not improve from 0.58159\n","Epoch 47/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.7005 - accuracy: 0.5273 - val_loss: 0.6886 - val_accuracy: 0.5523\n","\n","Epoch 00047: val_accuracy did not improve from 0.58159\n","Epoch 48/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7020 - accuracy: 0.5105 - val_loss: 0.6885 - val_accuracy: 0.5439\n","\n","Epoch 00048: val_accuracy did not improve from 0.58159\n","Epoch 49/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6947 - accuracy: 0.5335 - val_loss: 0.6887 - val_accuracy: 0.5439\n","\n","Epoch 00049: val_accuracy did not improve from 0.58159\n","Epoch 50/100\n","954/954 [==============================] - 0s 32us/step - loss: 0.6987 - accuracy: 0.5377 - val_loss: 0.6887 - val_accuracy: 0.5439\n","\n","Epoch 00050: val_accuracy did not improve from 0.58159\n","Epoch 51/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7039 - accuracy: 0.5199 - val_loss: 0.6887 - val_accuracy: 0.5439\n","\n","Epoch 00051: val_accuracy did not improve from 0.58159\n","Epoch 52/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6931 - accuracy: 0.5356 - val_loss: 0.6890 - val_accuracy: 0.5439\n","\n","Epoch 00052: val_accuracy did not improve from 0.58159\n","Epoch 53/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6868 - accuracy: 0.5503 - val_loss: 0.6893 - val_accuracy: 0.5439\n","\n","Epoch 00053: val_accuracy did not improve from 0.58159\n","Epoch 54/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6948 - accuracy: 0.5335 - val_loss: 0.6898 - val_accuracy: 0.5439\n","\n","Epoch 00054: val_accuracy did not improve from 0.58159\n","Epoch 55/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.7013 - accuracy: 0.5105 - val_loss: 0.6901 - val_accuracy: 0.5439\n","\n","Epoch 00055: val_accuracy did not improve from 0.58159\n","Epoch 56/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6965 - accuracy: 0.5199 - val_loss: 0.6903 - val_accuracy: 0.5439\n","\n","Epoch 00056: val_accuracy did not improve from 0.58159\n","Epoch 57/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6913 - accuracy: 0.5451 - val_loss: 0.6907 - val_accuracy: 0.5397\n","\n","Epoch 00057: val_accuracy did not improve from 0.58159\n","Epoch 58/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.7030 - accuracy: 0.5231 - val_loss: 0.6909 - val_accuracy: 0.5356\n","\n","Epoch 00058: val_accuracy did not improve from 0.58159\n","Epoch 59/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.6927 - accuracy: 0.5461 - val_loss: 0.6909 - val_accuracy: 0.5356\n","\n","Epoch 00059: val_accuracy did not improve from 0.58159\n","Epoch 60/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.6957 - accuracy: 0.5294 - val_loss: 0.6908 - val_accuracy: 0.5356\n","\n","Epoch 00060: val_accuracy did not improve from 0.58159\n","Epoch 61/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.7033 - accuracy: 0.5199 - val_loss: 0.6906 - val_accuracy: 0.5356\n","\n","Epoch 00061: val_accuracy did not improve from 0.58159\n","Epoch 62/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.7005 - accuracy: 0.5189 - val_loss: 0.6908 - val_accuracy: 0.5397\n","\n","Epoch 00062: val_accuracy did not improve from 0.58159\n","Epoch 63/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6950 - accuracy: 0.5430 - val_loss: 0.6909 - val_accuracy: 0.5439\n","\n","Epoch 00063: val_accuracy did not improve from 0.58159\n","Epoch 64/100\n","954/954 [==============================] - 0s 31us/step - loss: 0.7004 - accuracy: 0.5189 - val_loss: 0.6910 - val_accuracy: 0.5397\n","\n","Epoch 00064: val_accuracy did not improve from 0.58159\n","Epoch 65/100\n","954/954 [==============================] - 0s 31us/step - loss: 0.6979 - accuracy: 0.5199 - val_loss: 0.6911 - val_accuracy: 0.5314\n","\n","Epoch 00065: val_accuracy did not improve from 0.58159\n","Epoch 66/100\n","954/954 [==============================] - 0s 32us/step - loss: 0.6954 - accuracy: 0.5241 - val_loss: 0.6914 - val_accuracy: 0.5314\n","\n","Epoch 00066: val_accuracy did not improve from 0.58159\n","Epoch 67/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.7081 - accuracy: 0.4738 - val_loss: 0.6915 - val_accuracy: 0.5314\n","\n","Epoch 00067: val_accuracy did not improve from 0.58159\n","Epoch 68/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.6974 - accuracy: 0.5325 - val_loss: 0.6916 - val_accuracy: 0.5314\n","\n","Epoch 00068: val_accuracy did not improve from 0.58159\n","Epoch 69/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6961 - accuracy: 0.5273 - val_loss: 0.6915 - val_accuracy: 0.5272\n","\n","Epoch 00069: val_accuracy did not improve from 0.58159\n","Epoch 70/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.6938 - accuracy: 0.5294 - val_loss: 0.6915 - val_accuracy: 0.5356\n","\n","Epoch 00070: val_accuracy did not improve from 0.58159\n","Epoch 71/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.6961 - accuracy: 0.5273 - val_loss: 0.6915 - val_accuracy: 0.5314\n","\n","Epoch 00071: val_accuracy did not improve from 0.58159\n","Epoch 72/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6862 - accuracy: 0.5482 - val_loss: 0.6916 - val_accuracy: 0.5314\n","\n","Epoch 00072: val_accuracy did not improve from 0.58159\n","Epoch 73/100\n","954/954 [==============================] - 0s 31us/step - loss: 0.6943 - accuracy: 0.5231 - val_loss: 0.6917 - val_accuracy: 0.5314\n","\n","Epoch 00073: val_accuracy did not improve from 0.58159\n","Epoch 74/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6961 - accuracy: 0.5210 - val_loss: 0.6920 - val_accuracy: 0.5356\n","\n","Epoch 00074: val_accuracy did not improve from 0.58159\n","Epoch 75/100\n","954/954 [==============================] - 0s 36us/step - loss: 0.6892 - accuracy: 0.5493 - val_loss: 0.6922 - val_accuracy: 0.5356\n","\n","Epoch 00075: val_accuracy did not improve from 0.58159\n","Epoch 76/100\n","954/954 [==============================] - 0s 34us/step - loss: 0.6974 - accuracy: 0.5094 - val_loss: 0.6922 - val_accuracy: 0.5356\n","\n","Epoch 00076: val_accuracy did not improve from 0.58159\n","Epoch 77/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6917 - accuracy: 0.5147 - val_loss: 0.6920 - val_accuracy: 0.5397\n","\n","Epoch 00077: val_accuracy did not improve from 0.58159\n","Epoch 78/100\n","954/954 [==============================] - 0s 34us/step - loss: 0.6925 - accuracy: 0.5472 - val_loss: 0.6919 - val_accuracy: 0.5397\n","\n","Epoch 00078: val_accuracy did not improve from 0.58159\n","Epoch 79/100\n","954/954 [==============================] - 0s 32us/step - loss: 0.6968 - accuracy: 0.5063 - val_loss: 0.6917 - val_accuracy: 0.5356\n","\n","Epoch 00079: val_accuracy did not improve from 0.58159\n","Epoch 80/100\n","954/954 [==============================] - 0s 32us/step - loss: 0.7008 - accuracy: 0.5157 - val_loss: 0.6916 - val_accuracy: 0.5397\n","\n","Epoch 00080: val_accuracy did not improve from 0.58159\n","Epoch 81/100\n","954/954 [==============================] - 0s 31us/step - loss: 0.6968 - accuracy: 0.5252 - val_loss: 0.6914 - val_accuracy: 0.5439\n","\n","Epoch 00081: val_accuracy did not improve from 0.58159\n","Epoch 82/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6989 - accuracy: 0.5283 - val_loss: 0.6913 - val_accuracy: 0.5439\n","\n","Epoch 00082: val_accuracy did not improve from 0.58159\n","Epoch 83/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6931 - accuracy: 0.5189 - val_loss: 0.6911 - val_accuracy: 0.5439\n","\n","Epoch 00083: val_accuracy did not improve from 0.58159\n","Epoch 84/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6978 - accuracy: 0.5220 - val_loss: 0.6909 - val_accuracy: 0.5439\n","\n","Epoch 00084: val_accuracy did not improve from 0.58159\n","Epoch 85/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6898 - accuracy: 0.5262 - val_loss: 0.6908 - val_accuracy: 0.5439\n","\n","Epoch 00085: val_accuracy did not improve from 0.58159\n","Epoch 86/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.7007 - accuracy: 0.5000 - val_loss: 0.6907 - val_accuracy: 0.5439\n","\n","Epoch 00086: val_accuracy did not improve from 0.58159\n","Epoch 87/100\n","954/954 [==============================] - 0s 32us/step - loss: 0.6938 - accuracy: 0.5335 - val_loss: 0.6906 - val_accuracy: 0.5439\n","\n","Epoch 00087: val_accuracy did not improve from 0.58159\n","Epoch 88/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.6931 - accuracy: 0.5273 - val_loss: 0.6906 - val_accuracy: 0.5439\n","\n","Epoch 00088: val_accuracy did not improve from 0.58159\n","Epoch 89/100\n","954/954 [==============================] - 0s 37us/step - loss: 0.6971 - accuracy: 0.5335 - val_loss: 0.6906 - val_accuracy: 0.5439\n","\n","Epoch 00089: val_accuracy did not improve from 0.58159\n","Epoch 90/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.6896 - accuracy: 0.5367 - val_loss: 0.6906 - val_accuracy: 0.5397\n","\n","Epoch 00090: val_accuracy did not improve from 0.58159\n","Epoch 91/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.6954 - accuracy: 0.5199 - val_loss: 0.6906 - val_accuracy: 0.5397\n","\n","Epoch 00091: val_accuracy did not improve from 0.58159\n","Epoch 92/100\n","954/954 [==============================] - 0s 33us/step - loss: 0.6950 - accuracy: 0.5199 - val_loss: 0.6908 - val_accuracy: 0.5397\n","\n","Epoch 00092: val_accuracy did not improve from 0.58159\n","Epoch 93/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6950 - accuracy: 0.5304 - val_loss: 0.6909 - val_accuracy: 0.5356\n","\n","Epoch 00093: val_accuracy did not improve from 0.58159\n","Epoch 94/100\n","954/954 [==============================] - 0s 29us/step - loss: 0.6922 - accuracy: 0.5294 - val_loss: 0.6911 - val_accuracy: 0.5439\n","\n","Epoch 00094: val_accuracy did not improve from 0.58159\n","Epoch 95/100\n","954/954 [==============================] - 0s 31us/step - loss: 0.6935 - accuracy: 0.5136 - val_loss: 0.6912 - val_accuracy: 0.5439\n","\n","Epoch 00095: val_accuracy did not improve from 0.58159\n","Epoch 96/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6947 - accuracy: 0.5503 - val_loss: 0.6913 - val_accuracy: 0.5439\n","\n","Epoch 00096: val_accuracy did not improve from 0.58159\n","Epoch 97/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6945 - accuracy: 0.5283 - val_loss: 0.6915 - val_accuracy: 0.5397\n","\n","Epoch 00097: val_accuracy did not improve from 0.58159\n","Epoch 98/100\n","954/954 [==============================] - 0s 37us/step - loss: 0.6928 - accuracy: 0.5356 - val_loss: 0.6917 - val_accuracy: 0.5439\n","\n","Epoch 00098: val_accuracy did not improve from 0.58159\n","Epoch 99/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6977 - accuracy: 0.5409 - val_loss: 0.6919 - val_accuracy: 0.5439\n","\n","Epoch 00099: val_accuracy did not improve from 0.58159\n","Epoch 100/100\n","954/954 [==============================] - 0s 30us/step - loss: 0.6953 - accuracy: 0.5115 - val_loss: 0.6919 - val_accuracy: 0.5439\n","\n","Epoch 00100: val_accuracy did not improve from 0.58159\n","796/796 [==============================] - 0s 12us/step\n","[0.6922056393407697, 0.5351758599281311]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aFUh96vVBoda","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593418692016,"user_tz":240,"elapsed":561,"user":{"displayName":"varun vallabhan","photoUrl":"","userId":"05449009114552710154"}},"outputId":"fbf4a72c-e210-4eb2-8fcf-b90bd2207910"},"source":["model.load_weights(\"weights-improvement-05-0.57.hdf5\")\n","print(model.evaluate(X_test,y_test,batch_size=128))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["796/796 [==============================] - 0s 14us/step\n","[0.6915075704080975, 0.536432147026062]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cd0Dv-62JocA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1591864915339,"user_tz":240,"elapsed":94738,"user":{"displayName":"varun vallabhan","photoUrl":"","userId":"05449009114552710154"}},"outputId":"b25f6267-ebb9-489b-8737-dbc3d2f27560"},"source":["\"\"\"\n","This is where I have built my model. This a MLP\n","model which saves the best weights WRT to Validation \n","accuracy. The model includes blocks of Dense, Batch, \n","Dropout layers initially increasing in number of nodes \n","and then converging to single node to predict 0,1.\n","The model uses 20% validation split. \n","\"\"\"\n","def create_model(units,drop):\n","  model = Sequential()\n","\n","  #model.add(Dropout(0.2))\n","  model.add(Dense(units, activation='relu', input_dim=25))\n","  model.add(Dropout(0.2))\n","  model.add(BatchNormalization())\n","  model.add(Dense(units*2, activation='relu'))\n","  model.add(Dropout(drop))\n","  model.add(BatchNormalization())\n","  \"\"\"\n","  model.add(Dense(units*4,activation='relu'))\n","  #model.add(Dropout(drop))\n","  model.add(BatchNormalization())\n","  model.add(Dense(units*8, activation='relu'))\n","  #model.add(Dropout(drop))\n","  model.add(BatchNormalization())\n","  model.add(Dense(units*4,activation='relu'))\n","  #model.add(Dropout(drop))\n","  model.add(BatchNormalization())\n","  \"\"\"\n","  model.add(Dense(units*2, activation='relu'))\n","  model.add(Dropout(drop))\n","  model.add(BatchNormalization())\n","  model.add(Dense(units, activation='relu'))\n","  model.add(Dropout(drop))\n","  model.add(BatchNormalization())\n","  model.add(Dense(100,activation='relu'))\n","  model.add(Dense(1,activation='sigmoid'))\n","  \n","  model.compile(loss=\"binary_crossentropy\", optimizer='adam',metrics=[\"accuracy\"])\n","  print(model.summary())\n","  return model\n","\n","model=create_model(256,drop=0.2)\n","filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n","callbacks_list = [checkpoint]\n","  \n","model.fit(X_train,y_train,epochs=100,validation_split=0.2,batch_size=12,callbacks=callbacks_list)\n","print(model.evaluate(X_test,y_test,batch_size=128))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_7 (Dense)              (None, 256)               6656      \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 256)               1024      \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 512)               131584    \n","_________________________________________________________________\n","dropout_7 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 512)               2048      \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 512)               262656    \n","_________________________________________________________________\n","dropout_8 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 512)               2048      \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 256)               131328    \n","_________________________________________________________________\n","dropout_9 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","batch_normalization_8 (Batch (None, 256)               1024      \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 100)               25700     \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 1)                 101       \n","=================================================================\n","Total params: 564,169\n","Trainable params: 561,097\n","Non-trainable params: 3,072\n","_________________________________________________________________\n","None\n","Train on 954 samples, validate on 239 samples\n","Epoch 1/100\n","954/954 [==============================] - 2s 2ms/step - loss: 0.8175 - accuracy: 0.5073 - val_loss: 0.6925 - val_accuracy: 0.5523\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.55230, saving model to weights-improvement-01-0.55.hdf5\n","Epoch 2/100\n","954/954 [==============================] - 1s 966us/step - loss: 0.7628 - accuracy: 0.5136 - val_loss: 0.6897 - val_accuracy: 0.5397\n","\n","Epoch 00002: val_accuracy did not improve from 0.55230\n","Epoch 3/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.7252 - accuracy: 0.5241 - val_loss: 0.6942 - val_accuracy: 0.5481\n","\n","Epoch 00003: val_accuracy did not improve from 0.55230\n","Epoch 4/100\n","954/954 [==============================] - 1s 930us/step - loss: 0.6974 - accuracy: 0.5744 - val_loss: 0.7044 - val_accuracy: 0.5314\n","\n","Epoch 00004: val_accuracy did not improve from 0.55230\n","Epoch 5/100\n","954/954 [==============================] - 1s 930us/step - loss: 0.6696 - accuracy: 0.6132 - val_loss: 0.7037 - val_accuracy: 0.5774\n","\n","Epoch 00005: val_accuracy improved from 0.55230 to 0.57741, saving model to weights-improvement-05-0.58.hdf5\n","Epoch 6/100\n","954/954 [==============================] - 1s 965us/step - loss: 0.6864 - accuracy: 0.5870 - val_loss: 0.7430 - val_accuracy: 0.4979\n","\n","Epoch 00006: val_accuracy did not improve from 0.57741\n","Epoch 7/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.6667 - accuracy: 0.6289 - val_loss: 0.7525 - val_accuracy: 0.4937\n","\n","Epoch 00007: val_accuracy did not improve from 0.57741\n","Epoch 8/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.6600 - accuracy: 0.6258 - val_loss: 0.7477 - val_accuracy: 0.5314\n","\n","Epoch 00008: val_accuracy did not improve from 0.57741\n","Epoch 9/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.6120 - accuracy: 0.6719 - val_loss: 0.8192 - val_accuracy: 0.5188\n","\n","Epoch 00009: val_accuracy did not improve from 0.57741\n","Epoch 10/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.6017 - accuracy: 0.6625 - val_loss: 0.8050 - val_accuracy: 0.5021\n","\n","Epoch 00010: val_accuracy did not improve from 0.57741\n","Epoch 11/100\n","954/954 [==============================] - 1s 927us/step - loss: 0.5821 - accuracy: 0.6866 - val_loss: 0.8626 - val_accuracy: 0.5188\n","\n","Epoch 00011: val_accuracy did not improve from 0.57741\n","Epoch 12/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.5495 - accuracy: 0.7243 - val_loss: 0.9195 - val_accuracy: 0.5230\n","\n","Epoch 00012: val_accuracy did not improve from 0.57741\n","Epoch 13/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.5361 - accuracy: 0.7348 - val_loss: 0.8994 - val_accuracy: 0.5314\n","\n","Epoch 00013: val_accuracy did not improve from 0.57741\n","Epoch 14/100\n","954/954 [==============================] - 1s 942us/step - loss: 0.5595 - accuracy: 0.7138 - val_loss: 0.9267 - val_accuracy: 0.5063\n","\n","Epoch 00014: val_accuracy did not improve from 0.57741\n","Epoch 15/100\n","954/954 [==============================] - 1s 897us/step - loss: 0.5102 - accuracy: 0.7421 - val_loss: 0.9628 - val_accuracy: 0.5021\n","\n","Epoch 00015: val_accuracy did not improve from 0.57741\n","Epoch 16/100\n","954/954 [==============================] - 1s 910us/step - loss: 0.4951 - accuracy: 0.7610 - val_loss: 0.9954 - val_accuracy: 0.5314\n","\n","Epoch 00016: val_accuracy did not improve from 0.57741\n","Epoch 17/100\n","954/954 [==============================] - 1s 913us/step - loss: 0.4742 - accuracy: 0.7610 - val_loss: 0.9790 - val_accuracy: 0.5188\n","\n","Epoch 00017: val_accuracy did not improve from 0.57741\n","Epoch 18/100\n","954/954 [==============================] - 1s 927us/step - loss: 0.4834 - accuracy: 0.7683 - val_loss: 0.9690 - val_accuracy: 0.5230\n","\n","Epoch 00018: val_accuracy did not improve from 0.57741\n","Epoch 19/100\n","954/954 [==============================] - 1s 948us/step - loss: 0.4727 - accuracy: 0.7704 - val_loss: 0.9598 - val_accuracy: 0.5481\n","\n","Epoch 00019: val_accuracy did not improve from 0.57741\n","Epoch 20/100\n","954/954 [==============================] - 1s 908us/step - loss: 0.4348 - accuracy: 0.7945 - val_loss: 1.0334 - val_accuracy: 0.5063\n","\n","Epoch 00020: val_accuracy did not improve from 0.57741\n","Epoch 21/100\n","954/954 [==============================] - 1s 913us/step - loss: 0.4148 - accuracy: 0.8008 - val_loss: 1.1695 - val_accuracy: 0.5105\n","\n","Epoch 00021: val_accuracy did not improve from 0.57741\n","Epoch 22/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.3873 - accuracy: 0.8260 - val_loss: 1.0524 - val_accuracy: 0.5607\n","\n","Epoch 00022: val_accuracy did not improve from 0.57741\n","Epoch 23/100\n","954/954 [==============================] - 1s 999us/step - loss: 0.3995 - accuracy: 0.8281 - val_loss: 1.1149 - val_accuracy: 0.5230\n","\n","Epoch 00023: val_accuracy did not improve from 0.57741\n","Epoch 24/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.3563 - accuracy: 0.8428 - val_loss: 1.1770 - val_accuracy: 0.4937\n","\n","Epoch 00024: val_accuracy did not improve from 0.57741\n","Epoch 25/100\n","954/954 [==============================] - 1s 911us/step - loss: 0.3793 - accuracy: 0.8260 - val_loss: 1.1044 - val_accuracy: 0.5146\n","\n","Epoch 00025: val_accuracy did not improve from 0.57741\n","Epoch 26/100\n","954/954 [==============================] - 1s 888us/step - loss: 0.3818 - accuracy: 0.8449 - val_loss: 1.0881 - val_accuracy: 0.5063\n","\n","Epoch 00026: val_accuracy did not improve from 0.57741\n","Epoch 27/100\n","954/954 [==============================] - 1s 916us/step - loss: 0.3488 - accuracy: 0.8417 - val_loss: 1.2376 - val_accuracy: 0.5105\n","\n","Epoch 00027: val_accuracy did not improve from 0.57741\n","Epoch 28/100\n","954/954 [==============================] - 1s 904us/step - loss: 0.3166 - accuracy: 0.8648 - val_loss: 1.2211 - val_accuracy: 0.5523\n","\n","Epoch 00028: val_accuracy did not improve from 0.57741\n","Epoch 29/100\n","954/954 [==============================] - 1s 932us/step - loss: 0.3022 - accuracy: 0.8658 - val_loss: 1.2387 - val_accuracy: 0.5146\n","\n","Epoch 00029: val_accuracy did not improve from 0.57741\n","Epoch 30/100\n","954/954 [==============================] - 1s 997us/step - loss: 0.3517 - accuracy: 0.8449 - val_loss: 1.2764 - val_accuracy: 0.5188\n","\n","Epoch 00030: val_accuracy did not improve from 0.57741\n","Epoch 31/100\n","954/954 [==============================] - 1s 901us/step - loss: 0.3458 - accuracy: 0.8438 - val_loss: 1.3127 - val_accuracy: 0.5188\n","\n","Epoch 00031: val_accuracy did not improve from 0.57741\n","Epoch 32/100\n","954/954 [==============================] - 1s 924us/step - loss: 0.2834 - accuracy: 0.8784 - val_loss: 1.3313 - val_accuracy: 0.4979\n","\n","Epoch 00032: val_accuracy did not improve from 0.57741\n","Epoch 33/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.2988 - accuracy: 0.8795 - val_loss: 1.3590 - val_accuracy: 0.4854\n","\n","Epoch 00033: val_accuracy did not improve from 0.57741\n","Epoch 34/100\n","954/954 [==============================] - 1s 908us/step - loss: 0.3575 - accuracy: 0.8449 - val_loss: 1.1849 - val_accuracy: 0.5105\n","\n","Epoch 00034: val_accuracy did not improve from 0.57741\n","Epoch 35/100\n","954/954 [==============================] - 1s 912us/step - loss: 0.2902 - accuracy: 0.8836 - val_loss: 1.2923 - val_accuracy: 0.4728\n","\n","Epoch 00035: val_accuracy did not improve from 0.57741\n","Epoch 36/100\n","954/954 [==============================] - 1s 922us/step - loss: 0.2932 - accuracy: 0.8816 - val_loss: 1.2757 - val_accuracy: 0.4937\n","\n","Epoch 00036: val_accuracy did not improve from 0.57741\n","Epoch 37/100\n","954/954 [==============================] - 1s 938us/step - loss: 0.2667 - accuracy: 0.8868 - val_loss: 1.3412 - val_accuracy: 0.5272\n","\n","Epoch 00037: val_accuracy did not improve from 0.57741\n","Epoch 38/100\n","954/954 [==============================] - 1s 917us/step - loss: 0.2554 - accuracy: 0.8878 - val_loss: 1.4791 - val_accuracy: 0.4854\n","\n","Epoch 00038: val_accuracy did not improve from 0.57741\n","Epoch 39/100\n","954/954 [==============================] - 1s 918us/step - loss: 0.2817 - accuracy: 0.8847 - val_loss: 1.4360 - val_accuracy: 0.4854\n","\n","Epoch 00039: val_accuracy did not improve from 0.57741\n","Epoch 40/100\n","954/954 [==============================] - 1s 916us/step - loss: 0.2664 - accuracy: 0.8931 - val_loss: 1.4887 - val_accuracy: 0.5021\n","\n","Epoch 00040: val_accuracy did not improve from 0.57741\n","Epoch 41/100\n","954/954 [==============================] - 1s 922us/step - loss: 0.2856 - accuracy: 0.8826 - val_loss: 1.3542 - val_accuracy: 0.5188\n","\n","Epoch 00041: val_accuracy did not improve from 0.57741\n","Epoch 42/100\n","954/954 [==============================] - 1s 986us/step - loss: 0.2610 - accuracy: 0.8920 - val_loss: 1.5317 - val_accuracy: 0.5146\n","\n","Epoch 00042: val_accuracy did not improve from 0.57741\n","Epoch 43/100\n","954/954 [==============================] - 1s 919us/step - loss: 0.2398 - accuracy: 0.9015 - val_loss: 1.4620 - val_accuracy: 0.5230\n","\n","Epoch 00043: val_accuracy did not improve from 0.57741\n","Epoch 44/100\n","954/954 [==============================] - 1s 929us/step - loss: 0.2884 - accuracy: 0.8711 - val_loss: 1.4270 - val_accuracy: 0.4812\n","\n","Epoch 00044: val_accuracy did not improve from 0.57741\n","Epoch 45/100\n","954/954 [==============================] - 1s 921us/step - loss: 0.2636 - accuracy: 0.8973 - val_loss: 1.3580 - val_accuracy: 0.5523\n","\n","Epoch 00045: val_accuracy did not improve from 0.57741\n","Epoch 46/100\n","954/954 [==============================] - 1s 956us/step - loss: 0.2382 - accuracy: 0.9004 - val_loss: 1.4175 - val_accuracy: 0.5063\n","\n","Epoch 00046: val_accuracy did not improve from 0.57741\n","Epoch 47/100\n","954/954 [==============================] - 1s 999us/step - loss: 0.2435 - accuracy: 0.9099 - val_loss: 1.5245 - val_accuracy: 0.4770\n","\n","Epoch 00047: val_accuracy did not improve from 0.57741\n","Epoch 48/100\n","954/954 [==============================] - 1s 986us/step - loss: 0.2187 - accuracy: 0.9130 - val_loss: 1.4929 - val_accuracy: 0.4854\n","\n","Epoch 00048: val_accuracy did not improve from 0.57741\n","Epoch 49/100\n","954/954 [==============================] - 1s 958us/step - loss: 0.2319 - accuracy: 0.9078 - val_loss: 1.5185 - val_accuracy: 0.4979\n","\n","Epoch 00049: val_accuracy did not improve from 0.57741\n","Epoch 50/100\n","954/954 [==============================] - 1s 921us/step - loss: 0.2200 - accuracy: 0.9140 - val_loss: 1.5481 - val_accuracy: 0.4770\n","\n","Epoch 00050: val_accuracy did not improve from 0.57741\n","Epoch 51/100\n","954/954 [==============================] - 1s 913us/step - loss: 0.2007 - accuracy: 0.9266 - val_loss: 1.5927 - val_accuracy: 0.5063\n","\n","Epoch 00051: val_accuracy did not improve from 0.57741\n","Epoch 52/100\n","954/954 [==============================] - 1s 899us/step - loss: 0.2231 - accuracy: 0.9046 - val_loss: 1.5984 - val_accuracy: 0.5105\n","\n","Epoch 00052: val_accuracy did not improve from 0.57741\n","Epoch 53/100\n","954/954 [==============================] - 1s 939us/step - loss: 0.1954 - accuracy: 0.9088 - val_loss: 1.7181 - val_accuracy: 0.4937\n","\n","Epoch 00053: val_accuracy did not improve from 0.57741\n","Epoch 54/100\n","954/954 [==============================] - 1s 953us/step - loss: 0.2492 - accuracy: 0.9046 - val_loss: 1.3493 - val_accuracy: 0.5063\n","\n","Epoch 00054: val_accuracy did not improve from 0.57741\n","Epoch 55/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.2003 - accuracy: 0.9224 - val_loss: 1.5312 - val_accuracy: 0.5146\n","\n","Epoch 00055: val_accuracy did not improve from 0.57741\n","Epoch 56/100\n","954/954 [==============================] - 1s 935us/step - loss: 0.2167 - accuracy: 0.9182 - val_loss: 1.4341 - val_accuracy: 0.5146\n","\n","Epoch 00056: val_accuracy did not improve from 0.57741\n","Epoch 57/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.2083 - accuracy: 0.9099 - val_loss: 1.6388 - val_accuracy: 0.4895\n","\n","Epoch 00057: val_accuracy did not improve from 0.57741\n","Epoch 58/100\n","954/954 [==============================] - 1s 953us/step - loss: 0.2119 - accuracy: 0.9130 - val_loss: 1.6592 - val_accuracy: 0.4895\n","\n","Epoch 00058: val_accuracy did not improve from 0.57741\n","Epoch 59/100\n","954/954 [==============================] - 1s 959us/step - loss: 0.2154 - accuracy: 0.9057 - val_loss: 1.6086 - val_accuracy: 0.4937\n","\n","Epoch 00059: val_accuracy did not improve from 0.57741\n","Epoch 60/100\n","954/954 [==============================] - 1s 921us/step - loss: 0.1720 - accuracy: 0.9340 - val_loss: 1.6759 - val_accuracy: 0.5105\n","\n","Epoch 00060: val_accuracy did not improve from 0.57741\n","Epoch 61/100\n","954/954 [==============================] - 1s 906us/step - loss: 0.1995 - accuracy: 0.9245 - val_loss: 1.6453 - val_accuracy: 0.4895\n","\n","Epoch 00061: val_accuracy did not improve from 0.57741\n","Epoch 62/100\n","954/954 [==============================] - 1s 913us/step - loss: 0.1893 - accuracy: 0.9319 - val_loss: 1.5450 - val_accuracy: 0.5063\n","\n","Epoch 00062: val_accuracy did not improve from 0.57741\n","Epoch 63/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.1758 - accuracy: 0.9224 - val_loss: 1.6479 - val_accuracy: 0.5021\n","\n","Epoch 00063: val_accuracy did not improve from 0.57741\n","Epoch 64/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.1828 - accuracy: 0.9161 - val_loss: 1.7068 - val_accuracy: 0.5021\n","\n","Epoch 00064: val_accuracy did not improve from 0.57741\n","Epoch 65/100\n","954/954 [==============================] - 1s 916us/step - loss: 0.1992 - accuracy: 0.9214 - val_loss: 1.6541 - val_accuracy: 0.5063\n","\n","Epoch 00065: val_accuracy did not improve from 0.57741\n","Epoch 66/100\n","954/954 [==============================] - 1s 923us/step - loss: 0.1818 - accuracy: 0.9256 - val_loss: 1.7723 - val_accuracy: 0.5063\n","\n","Epoch 00066: val_accuracy did not improve from 0.57741\n","Epoch 67/100\n","954/954 [==============================] - 1s 901us/step - loss: 0.1859 - accuracy: 0.9319 - val_loss: 1.6600 - val_accuracy: 0.4603\n","\n","Epoch 00067: val_accuracy did not improve from 0.57741\n","Epoch 68/100\n","954/954 [==============================] - 1s 931us/step - loss: 0.1852 - accuracy: 0.9245 - val_loss: 1.7070 - val_accuracy: 0.4979\n","\n","Epoch 00068: val_accuracy did not improve from 0.57741\n","Epoch 69/100\n","954/954 [==============================] - 1s 996us/step - loss: 0.1570 - accuracy: 0.9277 - val_loss: 1.7219 - val_accuracy: 0.4854\n","\n","Epoch 00069: val_accuracy did not improve from 0.57741\n","Epoch 70/100\n","954/954 [==============================] - 1s 961us/step - loss: 0.1747 - accuracy: 0.9340 - val_loss: 1.6718 - val_accuracy: 0.4979\n","\n","Epoch 00070: val_accuracy did not improve from 0.57741\n","Epoch 71/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.1614 - accuracy: 0.9329 - val_loss: 1.7876 - val_accuracy: 0.4979\n","\n","Epoch 00071: val_accuracy did not improve from 0.57741\n","Epoch 72/100\n","954/954 [==============================] - 1s 949us/step - loss: 0.1675 - accuracy: 0.9319 - val_loss: 1.7376 - val_accuracy: 0.4937\n","\n","Epoch 00072: val_accuracy did not improve from 0.57741\n","Epoch 73/100\n","954/954 [==============================] - 1s 945us/step - loss: 0.1642 - accuracy: 0.9319 - val_loss: 1.6728 - val_accuracy: 0.5188\n","\n","Epoch 00073: val_accuracy did not improve from 0.57741\n","Epoch 74/100\n","954/954 [==============================] - 1s 930us/step - loss: 0.1793 - accuracy: 0.9340 - val_loss: 1.7768 - val_accuracy: 0.5272\n","\n","Epoch 00074: val_accuracy did not improve from 0.57741\n","Epoch 75/100\n","954/954 [==============================] - 1s 947us/step - loss: 0.1789 - accuracy: 0.9277 - val_loss: 1.6993 - val_accuracy: 0.5188\n","\n","Epoch 00075: val_accuracy did not improve from 0.57741\n","Epoch 76/100\n","954/954 [==============================] - 1s 980us/step - loss: 0.1791 - accuracy: 0.9308 - val_loss: 1.5935 - val_accuracy: 0.5439\n","\n","Epoch 00076: val_accuracy did not improve from 0.57741\n","Epoch 77/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.1629 - accuracy: 0.9329 - val_loss: 1.5638 - val_accuracy: 0.5230\n","\n","Epoch 00077: val_accuracy did not improve from 0.57741\n","Epoch 78/100\n","954/954 [==============================] - 1s 936us/step - loss: 0.1667 - accuracy: 0.9413 - val_loss: 1.7335 - val_accuracy: 0.5188\n","\n","Epoch 00078: val_accuracy did not improve from 0.57741\n","Epoch 79/100\n","954/954 [==============================] - 1s 978us/step - loss: 0.1775 - accuracy: 0.9319 - val_loss: 1.5841 - val_accuracy: 0.4644\n","\n","Epoch 00079: val_accuracy did not improve from 0.57741\n","Epoch 80/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.1837 - accuracy: 0.9298 - val_loss: 1.6718 - val_accuracy: 0.5063\n","\n","Epoch 00080: val_accuracy did not improve from 0.57741\n","Epoch 81/100\n","954/954 [==============================] - 1s 930us/step - loss: 0.1516 - accuracy: 0.9465 - val_loss: 1.6580 - val_accuracy: 0.5146\n","\n","Epoch 00081: val_accuracy did not improve from 0.57741\n","Epoch 82/100\n","954/954 [==============================] - 1s 899us/step - loss: 0.1490 - accuracy: 0.9403 - val_loss: 1.6766 - val_accuracy: 0.4937\n","\n","Epoch 00082: val_accuracy did not improve from 0.57741\n","Epoch 83/100\n","954/954 [==============================] - 1s 899us/step - loss: 0.2124 - accuracy: 0.9214 - val_loss: 1.5662 - val_accuracy: 0.4812\n","\n","Epoch 00083: val_accuracy did not improve from 0.57741\n","Epoch 84/100\n","954/954 [==============================] - 1s 921us/step - loss: 0.1706 - accuracy: 0.9382 - val_loss: 1.7864 - val_accuracy: 0.4979\n","\n","Epoch 00084: val_accuracy did not improve from 0.57741\n","Epoch 85/100\n","954/954 [==============================] - 1s 897us/step - loss: 0.1483 - accuracy: 0.9518 - val_loss: 1.6382 - val_accuracy: 0.4854\n","\n","Epoch 00085: val_accuracy did not improve from 0.57741\n","Epoch 86/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.1306 - accuracy: 0.9539 - val_loss: 1.7756 - val_accuracy: 0.5021\n","\n","Epoch 00086: val_accuracy did not improve from 0.57741\n","Epoch 87/100\n","954/954 [==============================] - 1s 941us/step - loss: 0.1411 - accuracy: 0.9486 - val_loss: 1.7565 - val_accuracy: 0.4937\n","\n","Epoch 00087: val_accuracy did not improve from 0.57741\n","Epoch 88/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.1701 - accuracy: 0.9382 - val_loss: 1.7289 - val_accuracy: 0.4979\n","\n","Epoch 00088: val_accuracy did not improve from 0.57741\n","Epoch 89/100\n","954/954 [==============================] - 1s 931us/step - loss: 0.1643 - accuracy: 0.9319 - val_loss: 1.6940 - val_accuracy: 0.5105\n","\n","Epoch 00089: val_accuracy did not improve from 0.57741\n","Epoch 90/100\n","954/954 [==============================] - 1s 917us/step - loss: 0.1639 - accuracy: 0.9340 - val_loss: 1.7973 - val_accuracy: 0.4895\n","\n","Epoch 00090: val_accuracy did not improve from 0.57741\n","Epoch 91/100\n","954/954 [==============================] - 1s 930us/step - loss: 0.1396 - accuracy: 0.9539 - val_loss: 1.7027 - val_accuracy: 0.4854\n","\n","Epoch 00091: val_accuracy did not improve from 0.57741\n","Epoch 92/100\n","954/954 [==============================] - 1s 928us/step - loss: 0.1290 - accuracy: 0.9539 - val_loss: 1.8297 - val_accuracy: 0.4979\n","\n","Epoch 00092: val_accuracy did not improve from 0.57741\n","Epoch 93/100\n","954/954 [==============================] - 1s 981us/step - loss: 0.1479 - accuracy: 0.9413 - val_loss: 1.7977 - val_accuracy: 0.4937\n","\n","Epoch 00093: val_accuracy did not improve from 0.57741\n","Epoch 94/100\n","954/954 [==============================] - 1s 945us/step - loss: 0.1192 - accuracy: 0.9497 - val_loss: 1.9716 - val_accuracy: 0.4728\n","\n","Epoch 00094: val_accuracy did not improve from 0.57741\n","Epoch 95/100\n","954/954 [==============================] - 1s 911us/step - loss: 0.1503 - accuracy: 0.9340 - val_loss: 2.0095 - val_accuracy: 0.4979\n","\n","Epoch 00095: val_accuracy did not improve from 0.57741\n","Epoch 96/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.1684 - accuracy: 0.9361 - val_loss: 1.7200 - val_accuracy: 0.4854\n","\n","Epoch 00096: val_accuracy did not improve from 0.57741\n","Epoch 97/100\n","954/954 [==============================] - 1s 920us/step - loss: 0.1160 - accuracy: 0.9591 - val_loss: 1.9132 - val_accuracy: 0.4770\n","\n","Epoch 00097: val_accuracy did not improve from 0.57741\n","Epoch 98/100\n","954/954 [==============================] - 1s 986us/step - loss: 0.1711 - accuracy: 0.9350 - val_loss: 1.7185 - val_accuracy: 0.5146\n","\n","Epoch 00098: val_accuracy did not improve from 0.57741\n","Epoch 99/100\n","954/954 [==============================] - 1s 1ms/step - loss: 0.1576 - accuracy: 0.9423 - val_loss: 1.7883 - val_accuracy: 0.4937\n","\n","Epoch 00099: val_accuracy did not improve from 0.57741\n","Epoch 100/100\n","954/954 [==============================] - 1s 916us/step - loss: 0.1866 - accuracy: 0.9235 - val_loss: 1.7672 - val_accuracy: 0.4979\n","\n","Epoch 00100: val_accuracy did not improve from 0.57741\n","796/796 [==============================] - 0s 52us/step\n","[1.7266640920734884, 0.49246230721473694]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9Mk6hFauJocJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":646},"executionInfo":{"status":"ok","timestamp":1591865127597,"user_tz":240,"elapsed":594,"user":{"displayName":"varun vallabhan","photoUrl":"","userId":"05449009114552710154"}},"outputId":"7e5cdbd4-b171-4895-f1a9-888ec8cee88b"},"source":["\"\"\"\n","This is a dummy model with same architecture to load pretrained \n","weights so that we can use the best trained weight set. One of the \n","best models is included in the drive in the weights folder. Which produces 74% accuracy.\n","\"\"\"\n","\n","\n","model=create_model(256,0.2)\n","model.load_weights(\"weights-improvement-05-0.58.hdf5\")\n","#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_13 (Dense)             (None, 256)               6656      \n","_________________________________________________________________\n","dropout_10 (Dropout)         (None, 256)               0         \n","_________________________________________________________________\n","batch_normalization_9 (Batch (None, 256)               1024      \n","_________________________________________________________________\n","dense_14 (Dense)             (None, 512)               131584    \n","_________________________________________________________________\n","dropout_11 (Dropout)         (None, 512)               0         \n","_________________________________________________________________\n","batch_normalization_10 (Batc (None, 512)               2048      \n","_________________________________________________________________\n","dense_15 (Dense)             (None, 512)               262656    \n","_________________________________________________________________\n","dropout_12 (Dropout)         (None, 512)               0         \n","_________________________________________________________________\n","batch_normalization_11 (Batc (None, 512)               2048      \n","_________________________________________________________________\n","dense_16 (Dense)             (None, 256)               131328    \n","_________________________________________________________________\n","dropout_13 (Dropout)         (None, 256)               0         \n","_________________________________________________________________\n","batch_normalization_12 (Batc (None, 256)               1024      \n","_________________________________________________________________\n","dense_17 (Dense)             (None, 100)               25700     \n","_________________________________________________________________\n","dense_18 (Dense)             (None, 1)                 101       \n","=================================================================\n","Total params: 564,169\n","Trainable params: 561,097\n","Non-trainable params: 3,072\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O6CA-hnstctc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1591865132990,"user_tz":240,"elapsed":508,"user":{"displayName":"varun vallabhan","photoUrl":"","userId":"05449009114552710154"}},"outputId":"4c7a90c7-58bc-43b6-c4b9-a06d2ec0c1d9"},"source":["\"\"\"\n","This part predicts and calculates the accuracy \n","and saves the value for the predictions for the \n","entire dataset for modifying the stock data to improve training.\n","\"\"\"\n","\n","print(model.evaluate(X_test,y_test,batch_size=128))\n","#out=model.predict(dat)\n","#out1=model.predict(X_train)\n","#print(clf.score(X_test,y_test))\n","#np.save('drive/My Drive/Summer 2020/Finance/data/stocknews/out.npy',out)\n","#print(out)\n","#np.save('drive/My Drive/downloads/stocknews/out_train.npy',out1)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["796/796 [==============================] - 0s 147us/step\n","[0.7304728255799068, 0.5100502371788025]\n"],"name":"stdout"}]}]}